---
title: "Project2Models6372NN"
author: "Nicole Norelli"
date: "3/31/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Creates train/test:
Sets up by adding year, re-factoring month, removing an outlier, removing 2008 data, and changing pdays -1 to 0. Then 80/20 train/test split.

Builds logistic regression models with a few feature selection tools
Calculates accuracy metrics and ROC curves.
Builds LDA/QDA model

Note: These are all a work in progress.  I'm not proposing any as a final model. But it's useful to have the code worked out while we try things

To do:
Try adding complexity
Work on normal distribution of continuous variables for LDA/QDA

```{r}
library(tidyverse)
# Bank Data Set
############## Description ###################
# The data is related with direct marketing campaigns of a Portuguese banking institution. 
# The marketing campaigns were based on phone calls. 
# Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be (or not) subscribed.
# From May 2008 to November 2010
# Goal: predict if client will subscribe a term deposit
# 45211 obs of 17 variables
# Description of variables:
##### Bank Client Data #######
# 1 - age (numeric)
# 2 - job : type of job (categorical: "admin.","unknown","unemployed","management","housemaid","entrepreneur","student","blue-collar","self-employed","retired","technician","services") 
# 3 - marital : marital status (categorical: "married","divorced","single"; note: "divorced" means divorced or widowed)
# 4 - education (categorical: "unknown","secondary","primary","tertiary")
# 5 - default: has credit in default? (binary: "yes","no")
# 6 - balance: average yearly balance, in euros (numeric) 
# 7 - housing: has housing loan? (binary: "yes","no")
# 8 - loan: has personal loan? (binary: "yes","no")
######### related with the last contact of the current campaign:######
# 9 - contact: contact communication type (categorical: "unknown","telephone","cellular") 
# 10 - day: last contact day of the month (numeric)
# 11 - month: last contact month of year (categorical: "jan", "feb", "mar", ..., "nov", "dec")
# 12 - duration: last contact duration, in seconds (numeric)
######## other attributes:#######
# 13 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)
# 14 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric, -1 means client was not previously contacted)
# 15 - previous: number of contacts performed before this campaign and for this client (numeric)
# 16 - poutcome: outcome of the previous marketing campaign (categorical: "unknown","other","failure","success")
#########Output variable (desired target):#######
# 17 - y - has the client subscribed a term deposit? (binary: "yes","no")

# Load CSV file:
# bank.full.csv file
bank.full <- read.csv("~/Downloads/bank/bank-full.csv", sep=";", stringsAsFactors=TRUE)
# Because these seem to be sequentially ordered, make a ID number in order
bank.full = bank.full %>%
  mutate(id = row_number())

# Now add in the year variable
bank.full = bank.full %>%
  mutate(year = case_when(id>=1 & id<=27729 ~ '2008',
                          id>=27730 & id<=42591 ~ '2009',
                          id>=42592 ~ '2010'))
bank.full$year = factor(bank.full$year)

# Let's reorder month so it makes sense on graphs
bank.full$month = factor(bank.full$month, levels=c("jan","feb","mar","apr","may","jun","jul","aug","sep","oct","nov","dec"))

# Change pdays -1 to 0:
bank.full$pdays[bank.full$pdays==-1] = 0

# Remove outlier (other in 2008)
bank.full = subset(bank.full, previous != 275)

# Remove 2008
bank.full.new = bank.full %>%
  filter(year != 2008)
# Drop level 2008
bank.full.new$year = droplevels(bank.full.new$year)
```

```{r}
# train test split
# 80/20 would be: 13985:3496
set.seed(1234)
index<-sample(1:dim(bank.full.new)[1],3496,replace=F)
test<-bank.full.new[index,]
train<-bank.full.new[-index,]
```

```{r}
# Build a logistic regression using glm() with all predictors to start
# Code from AutoClassify.R
library(ResourceSelection)
# Using glm
model.main<-glm(y ~ age+job+marital+education+default+balance+housing+loan+contact+day+month+duration+campaign+pdays+previous+poutcome+year, data=train,family = binomial(link="logit"))
(vif(model.main)[,3])^2
#Hosmer Lemeshow test for lack of fit.  Use as needed.  The g=10 is an option that deals with the continuous predictors if any are there.
#This should be increased with caution. 
hoslem.test(model.main$y, fitted(model.main), g=10)
# rejects. But the sample size is large, so I don't think it's an issue
#Summary of current fit
summary(model.main)
# Using the summary coefficients we can generate CI for each one in the table
exp(cbind("Odds ratio" = coef(model.main), confint.default(model.main, level = 0.95)))
# AIC: 10333
```

```{r}
#This starts with a null model and then builds up using forward selection up to all the predictors that were specified in main model previously.
# Code from AutoClassify.R
model.null<-glm(y ~ 1, data=train,family = binomial(link="logit"))
step(model.null,
     scope = list(upper=model.main),
     direction="forward",
     test="Chisq",
     data=train)
# Final model: y ~ month + duration + poutcome + year + housing + contact + day + education + campaign + loan + job + age
# AIC: 10330
```

```{r}
# Code from AutoClassify.R
# Take a look at the result
# Final model: y ~ month + duration + poutcome + year + housing + contact + day + education + campaign + loan + job + age
model.forward.1=glm(y~month + duration + poutcome + year + housing + contact + day + education + campaign + loan + job + age, data=train,family=binomial(link="logit"))
summary(model.forward.1)
exp(cbind("Odds ratio" = coef(model.forward.1), confint.default(model.forward.1, level = 0.95)))
vif(model.forward.1)
plot(model.forward.1)
# AIC: 10326
# if take out age for parsimony (not sig in final model) AIC: 10327
```

```{r}
# Code from HW 12
# Try stepwise selection using AIC as stopping criterion
library(MASS)
library(tidyverse)
full.log<-glm(y~age+job+marital+education+default+balance+housing+loan+contact+day+month+duration+campaign+pdays+previous+poutcome+year,family="binomial",data=train)
step.log<-full.log %>% stepAIC(trace=FALSE)
coef(step.log)
summary(step.log)
exp(cbind("Odds ratio" = coef(step.log), confint.default(step.log, level = 0.95)))
vif(step.log)
# model: y ~ age + job + education + housing + loan + contact + day + month + duration + campaign + poutcome + year
# same as above
# AIC: 10326
```

```{r}
# Code from HW 12
# Try Forward Selection
forward.log<-full.log %>% stepAIC(direction="forward",trace=FALSE)
coef(forward.log)
summary(forward.log)
exp(cbind("Odds ratio" = coef(forward.log), confint.default(forward.log, level = 0.95)))
vif(forward.log)
# model: y ~ age + job + marital + education + default + balance + housing + loan + contact + day + month + duration + campaign + pdays + previous + poutcome + year
# basically everything except response and id (which isn't included in any model, just for my reference)
# AIC: 10333
```

```{r}
# Code from HW 12
# Try Backward Selection
# I think backward is the default in stepAIC()
backward.log<-full.log %>% stepAIC(direction="backward",trace=FALSE)
coef(backward.log)
summary(backward.log)
exp(cbind("Odds ratio" = coef(backward.log), confint.default(backward.log, level = 0.95)))
vif(backward.log)
# model: y ~ age + job + education + housing + loan + contact + day + month + duration + campaign + poutcome + year
# AIC: 10326
```

```{r}
# Code from HW 12
# Try Both Selection
both.log<-full.log %>% stepAIC(direction="both",trace=FALSE)
coef(both.log)
summary(both.log)
exp(cbind("Odds ratio" = coef(both.log), confint.default(both.log, level = 0.95)))
vif(both.log)
# model: y ~ age + job + education + housing + loan + contact + day + month + duration + campaign + poutcome + year
# same as above
# AIC: 10326
```

```{r}
# Code from HW 12
# Try LASSO
# Cross validation is used to obtain the optimal penalty value.  A final refit using the entire data set can then be obtained once the optimal penalty value is determined.  For this example, the object "finalmodel" produces the final lasso model.
library(glmnet)
dat.train.x <- model.matrix(y~age+job+marital+education+default+balance+housing+loan+contact+day+month+duration+campaign+pdays+previous+poutcome+year-1,train)
dat.train.y<-train$y
cvfit <- cv.glmnet(dat.train.x, dat.train.y, family = "binomial", type.measure = "class", nlambda = 1000)
plot(cvfit)
coef(cvfit, s = "lambda.min")
#CV misclassification error rate is 0.1717555
print("CV Error Rate:")
cvfit$cvm[which(cvfit$lambda==cvfit$lambda.min)]

#Optimal penalty
print("Penalty Value:")
cvfit$lambda.min

#For final model predictions go ahead and refit lasso using entire data set
finalmodel<-glmnet(dat.train.x, dat.train.y, family = "binomial",lambda=cvfit$lambda.min)
# age + job + marital + education + balance + housing + loan + contact + day + month + duration + campaign +  poutcome + year
# basically all but previous, pdays, and default
# [1] "CV Error Rate:" 0.1703969
```

```{r}
# Code from glmnet_lda_code2_wRF.R
library(ROCR)
# NOTE: This is the ROC curve and predictions on the TRAIN data from the LASSO model above "finalmodel"

#Get training set predictions...We know they are biased but lets create ROC's.
#These are predicted probabilities from logistic model  exp(b)/(1+exp(b))
fit.pred <- predict(cvfit, newx = dat.train.x, type = "response")

#Compare the prediction to the real outcome
head(fit.pred)
head(dat.train.y)

#Create ROC curves
pred <- prediction(fit.pred[,1], dat.train.y)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")
auc.train <- auc.train@y.values

#Plot ROC
plot(roc.perf)
abline(a=0, b= 1) #Ref line indicating poor performance
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
# AUC 0.869
```

```{r}
# Code from HW12
# Make predictions on test set using LASSO model
dat.test.x<-model.matrix(y~age+job+marital+education+default+balance+housing+loan+contact+day+month+duration+campaign+pdays+previous+poutcome+year-1,test)
dat.test.y<-test$y
fit.pred.lasso <- predict(finalmodel, newx = dat.test.x, type = "response")

test$y[1:15]
fit.pred.lasso[1:15]

#Making predictions for stepwise as well
fit.pred.step<-predict(step.log,newdata=test,type="response")
```

```{r}
# Code from HW12
# These are error metrics for TEST set with LASSO and Stepwise models

#Lets use the predicted probabilities to classify the observations and make a final confusion matrix for the two models.  We can use it to calculate error metrics.
#Lets use a cutoff of 0.5 to make the classification.
cutoff<-0.5
class.lasso<-factor(ifelse(fit.pred.lasso>cutoff,"yes","no"),levels=c("no","yes"))
class.step<-factor(ifelse(fit.pred.step>cutoff,"yes","no"),levels=c("no","yes"))

#Confusion Matrix for Lasso
conf.lasso<-table(class.lasso,test$y)
print("Confusion matrix for LASSO")
conf.lasso

conf.step<-table(class.step,test$y)
print("Confusion matrix for Stepwise")
conf.step

#Accuracy of LASSO and Stepwise
print("Overall accuracy for LASSO and Stepwise respectively")
sum(diag(conf.lasso))/sum(conf.lasso)
sum(diag(conf.step))/sum(conf.step)

print("Alternative calculations of accuracy")
#Rather than making the calculations from the table, we can compute them more quickly using the following code which just checks if the prediction matches the truth and then computes the proportion.
mean(class.lasso==test$y)
mean(class.step==test$y)

# LASSO:
#class.lasso   no  yes
#        no  2537  435
#        yes  179  345
# overall accuracy: 0.8243707

# Stepwise:
#class.step   no  yes
#       no  2529  429
#       yes  187  351
# overall accuracy: 0.8237986

# Obviously these need some adjustment (ability to predict yes is not great) - look into better cutoff in ROC Curve
```

```{r}
# Code from glmnet_lda_code2_wRF.R
# ROC for test set

#ROC curves: LASSO model on test set
pred1 <- prediction(fit.pred.lasso[,1], dat.test.y)
roc.perf1 = performance(pred1, measure = "tpr", x.measure = "fpr")
auc.val1 <- performance(pred1, measure = "auc")
auc.val1 <- auc.val1@y.values
plot(roc.perf1, colorize=TRUE)
abline(a=0, b= 1)
text(x = .40, y = .6,paste("AUC = ", round(auc.val1[[1]],3), sep = ""))
# AUC=0.873.  Is there a problem?  I thought the training set should get a better AUC?

# Code from HW 12
#ROC curves: Step model on test set
results.step<-prediction(fit.pred.step, test$y,label.ordering=c("no","yes"))
roc.step = performance(results.step, measure = "tpr", x.measure = "fpr")
auc.val.step = performance(results.step, measure="auc")
auc.val.step = auc.val.step@y.values
plot(roc.step,colorize = TRUE)
abline(a=0, b= 1)
text(x = .40, y = .6,paste("AUC = ", round(auc.val.step[[1]],3), sep = ""))
# AUC=0.874
```

```{r}
# A look at different cut-off and accuracy metrics:
cutoff<-0.2
class.lasso<-factor(ifelse(fit.pred.lasso>cutoff,"yes","no"),levels=c("no","yes"))
class.step<-factor(ifelse(fit.pred.step>cutoff,"yes","no"),levels=c("no","yes"))

#Confusion Matrix for Lasso
conf.lasso<-table(class.lasso,test$y)
print("Confusion matrix for LASSO")
conf.lasso

conf.step<-table(class.step,test$y)
print("Confusion matrix for Stepwise")
conf.step

#Accuracy of LASSO and Stepwise
print("Overall accuracy for LASSO and Stepwise respectively")
sum(diag(conf.lasso))/sum(conf.lasso)
sum(diag(conf.step))/sum(conf.step)

# This number we choose depends on what percentage of yes's we want, obviously
```

```{r}
# Try a simpler version of step.log (w/o age because it's not sig)
step.log.1=glm(y ~ job + education + housing + loan + contact + day + month + duration + campaign + poutcome + year, data=train,family=binomial(link="logit"))
summary(step.log.1)
# AIC 10327
fit.pred.step.1<-predict(step.log.1,newdata=test,type="response")

cutoff<-0.5
class.step.1<-factor(ifelse(fit.pred.step.1>cutoff,"yes","no"),levels=c("no","yes"))
conf.step.1<-table(class.step.1,test$y)
print("Confusion matrix for Stepwise.1")
conf.step.1
print("Overall accuracy for Stepwise.1")
sum(diag(conf.step.1))/sum(conf.step.1)
# [1] "Confusion matrix for Stepwise.1"
#             
# class.step.1   no  yes
#          no  2529  426
#          yes  187  354
# [1] "Overall accuracy for Stepwise.1"
# [1] 0.8246568
         
#ROC curves: Step.1 model on test set
results.step.1<-prediction(fit.pred.step.1, test$y,label.ordering=c("no","yes"))
roc.step.1 = performance(results.step.1, measure = "tpr", x.measure = "fpr")
auc.val.step.1 = performance(results.step.1, measure="auc")
auc.val.step.1 = auc.val.step.1@y.values
plot(roc.step.1,colorize = TRUE)
abline(a=0, b= 1)
text(x = .40, y = .6,paste("AUC = ", round(auc.val.step.1[[1]],3), sep = ""))
title("Model 1 ROC Curve")
# AUC=0.875

#Hosmer Lemeshow test for lack of fit.  Use as needed.  The g=10 is an option that deals with the continuous predictors if any are there.
#This should be increased with caution. 
hoslem.test(step.log.1$y, fitted(step.log.1), g=10)
# Rejects (not a surprise.  data set is large)

plot(step.log.1)

summary(step.log.1)
# Using the summary coefficients we can generate CI for each one in the table
exp(cbind("Odds ratio" = coef(step.log.1), confint.default(step.log.1, level = 0.95)))
```

```{r}
# Code from glmnet_lda_code2_wRF.R
# NOTE: We need to work on normality. But I thought I'd get the code set up.
## LDA

#Training Set
lda.train.x <- train[,c(1,6,10,12,13,14,15)]
lda.train.y <- train$y

fit.lda <- lda(lda.train.y ~ ., data = lda.train.x)
pred.lda <- predict(fit.lda, newdata = lda.train.x)

preds <- pred.lda$posterior
preds <- as.data.frame(preds)

# ROC for TRAINING data
pred <- prediction(preds[,2],lda.train.y)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf, colorize=TRUE)
abline(a=0, b= 1)
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
# AUC=0.76. Not surprising. Maybe better if we normalize and then weed out some variables

# Try QDA
# Double check this Nicole
fit.qda <- qda(lda.train.y ~ ., data = lda.train.x)
pred.qda <- predict(fit.qda, newdata = lda.train.x)

preds <- pred.qda$posterior
preds <- as.data.frame(preds)

# ROC for TRAINING data
pred <- prediction(preds[,2],lda.train.y)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf, colorize=TRUE)
abline(a=0, b= 1)
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
# AUC: 0.73
```

```{r}
# Code from glmnet_lda_code2_wRF.R
#LDA for TEST data
lda.test.x <- test[,c(1,6,10,12,13,14,15)]
lda.test.y <- test$y

pred.lda1 <- predict(fit.lda, newdata = lda.test.x)

preds1 <- pred.lda1$posterior
preds1 <- as.data.frame(preds1)

pred1 <- prediction(preds1[,2],lda.test.y)
roc.perf = performance(pred1, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred1, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf, colorize=TRUE)
abline(a=0, b= 1)
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
# AUC = 0.775

# From HW8
# Confusion matrix for LDA on test data
pred.lda.cm<-predict(fit.lda,newdata=lda.test.x)$class  #Predictions can come in many forms, the class form provides the categorical level of your response.
Truth<-lda.test.y
x<-table(pred.lda.cm,Truth) # Creating a confusion matrix
x
#Missclassification Error
ME<-(x[2,1]+x[1,2])/3496  #change denom to N
ME
#Calculating overall accuracy
1-ME


# QDA for TEST data
pred.qda1 <- predict(fit.qda, newdata = lda.test.x)

preds1 <- pred.qda1$posterior
preds1 <- as.data.frame(preds1)

pred1 <- prediction(preds1[,2],lda.test.y)
roc.perf = performance(pred1, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred1, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf, colorize=TRUE)
abline(a=0, b= 1)
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
title("QDA ROC Curve")
# AUC = 0.731

# Confusion matrix for QDA on test data
pred.qda.cm<-predict(fit.qda,newdata=lda.test.x)$class  #Predictions can come in many forms, the class form provides the categorical level of your response.
Truth<-lda.test.y
x<-table(pred.qda.cm,Truth) # Creating a confusion matrix
x
#Missclassification Error
ME<-(x[2,1]+x[1,2])/3496  #change denom to N
ME
#Calculating overall accuracy
1-ME

#            Truth
# pred.qda.cm   no  yes
#         no  2504  558
#         yes  212  222
# [1] 0.2202517
# [1] 0.7797483
```

```{r}
# try some interactions
model.complex<-glm(y ~ age+job+marital+education+default+balance+housing+loan+contact+day+month+duration+campaign+pdays+previous+poutcome+year+year:month+year:day+I(duration^2)+I(pdays^2), data=train,family = binomial(link="logit"))
step(model.main,
     scope = list(upper=model.complex),
     direction="forward",
     test="Chisq",
     data=newAuto)
# Success! looks like yearxday and yearxmonth were significant
# AIC: 10010
# Try squaring each of the numerical variables
# Success! duration^2 is significant too
# AIC: 9730
# Success! also pdays^2 is significant
# AIC: 9726
```

```{r}
# Code from glmnet_lda_code2_wRF.R
library(randomForest)
# Random Forest (training data)

# Remove id variable as it's just for reference
dat.train.rf <- train[,-18]

train.rf<-randomForest(y~.,data=dat.train.rf,mtry=4,ntree=500,importance=T)
fit.pred<-predict(train.rf,newdata=dat.train.rf,type="prob")

pred <- prediction(fit.pred[,2], dat.train.rf$y)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf)
abline(a=0, b= 1)
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))

# confusion matrix for training data:
train.rf
# a look at variable importance
varImpPlot(train.rf)

# Try looking for optimal value of mtry
tuneRF(dat.train.rf[,-17], dat.train.rf[,17], stepfactor=1)
# indicates mtry=4 is best
```

```{r}
# Code from glmnet_lda_code2_wRF.R
# Random Forest (test data)

#Predict test set
dat.val1.rf <- test[,-18]

pred.val1<-predict(train.rf,newdata=dat.val1.rf,type="prob")

pred <- prediction(pred.val1[,2], dat.val1.rf$y)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf, colorize=TRUE)
abline(a=0, b= 1)
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
# AUC = 0.897
```

```{r}
# a look at the RF w/o year (for practical purposes)
dat.train.rf.1 <- train[,-(18:19)]

train.rf.1<-randomForest(y~.,data=dat.train.rf.1,mtry=4,ntree=500,importance=T)
dat.val1.rf.1 = test[,-(18:19)]
pred.val1.1<-predict(train.rf.1,newdata=dat.val1.rf.1,type="prob")
pred.rf.1 <- prediction(pred.val1.1[,2], dat.val1.rf.1$y)
roc.perf.rf.1 = performance(pred.rf.1, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred.rf.1, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf.rf.1, colorize=TRUE)
abline(a=0, b= 1)
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
title("Random Forest ROC Curve")
# AUC=.891  Not a huge decline

varImpPlot(train.rf.1, main="Variable Importance Plot for Random Forest Model")

# Try looking for optimal value of mtry
tuneRF(dat.train.rf.1[,-17], dat.train.rf.1[,17], stepfactor=1)
# indicates mtry=4 is best

# Confusion Matrix for train.rf.1 on test set data
#Making predictions on test and then observing accuracy rates
pred.rf.1.cm<-predict(train.rf.1,newdata=dat.val1.rf.1,type="response")
x=table(pred.rf.1.cm,dat.val1.rf.1$y) #Default prediction uses .5 as cut off you can change it specifying "cutoff" option

x
#Missclassification Error
ME<-(x[2,1]+x[1,2])/3496  #change denom to N
ME
#Calculating overall accuracy
1-ME

# pred.rf.1.cm   no  yes
#          no  2454  318
#          yes  262  462
# [1] 0.1659039
# [1] 0.8340961
```

```{r}
# Try adjusting the cut-off for train.rf.1

pred.rf.1.cm<-predict(train.rf.1,newdata=dat.val1.rf.1,type="response",cutoff=c(.7,1-.7))
x=table(pred.rf.1.cm,dat.val1.rf.1$y) #Default prediction uses .5 as cut off you can change it specifying "cutoff" option
x
ME<-(x[2,1]+x[1,2])/3496  #change denom to N
ME
#Calculating overall accuracy
1-ME
# I need to take a look at the sense/spec thing. cuz it seems backwards to me here.
# I thought yes's were the positives. so correct yes's would be sensitivity.
# But the AUC curve here looks opposite?
```

```{r}
# Code from HW 12
# ROC Curve comparison

# Step.log.1 (simple logistic model)
fit.pred.step.1<-predict(step.log.1,newdata=test,type="response")
results.step.1<-prediction(fit.pred.step.1, test$y,label.ordering=c("no","yes"))
roc.step = performance(results.step.1, measure = "tpr", x.measure = "fpr")


# Complex logistic (step.log.complex.noyr)
# Code in Project2ComplexModels6372NN
fit.pred.step.complex.noyr<-predict(step.log.complex.noyr,newdata=test,type="response")
results.step.complex<-prediction(fit.pred.step.complex.noyr, test$y,label.ordering=c("no","yes"))
roc.step.complex = performance(results.step.complex, measure = "tpr", x.measure = "fpr")

# QDA
pred.qda1 <- predict(fit.qda, newdata = lda.test.x)
preds1 <- pred.qda1$posterior
preds1 <- as.data.frame(preds1)
pred1 <- prediction(preds1[,2],lda.test.y)
roc.qda = performance(pred1, measure = "tpr", x.measure = "fpr")


# Random Forest (train.rf.1 - no year )
pred.val1.1<-predict(train.rf.1,newdata=dat.val1.rf.1,type="prob")
pred.rf.1 <- prediction(pred.val1.1[,2], dat.val1.rf.1$y)
roc.rf = performance(pred.rf.1, measure = "tpr", x.measure = "fpr")


plot(roc.step)
plot(roc.step.complex,col="orange", add = TRUE)
plot(roc.qda,col="blue",add=TRUE)
plot(roc.rf,col="green",add=TRUE)
legend("bottomright",legend=c("Simple Logistic (Model 1)","Complex Logistic (Model 2)", "QDA (Model 3)", "Random Forest (Model 4)"),col=c("black","orange","blue","green"),lty=1,lwd=1)
abline(a=0, b= 1)
title("ROC Model Comparisons")
```

```{r}
# Confusion matrix for random forest model on test data
pred.rf.val1.fortable = predict(train.rf,newdata=dat.val1.rf)
table(observed=dat.val1.rf$y,pred.rf.val1.fortable)
# maybe we will find out in class if there's a way to change the cutoff for this?
```