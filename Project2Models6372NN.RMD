---
title: "Project2Models6372NN"
author: "Nicole Norelli"
date: "3/31/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Creates train/test:
Sets up by adding year, re-factoring month, removing an outlier, removing 2008 data, and changing pdays -1 to 0. Then 80/20 train/test split.

Builds logistic regression models with a few feature selection tools
Calculates accuracy metrics and ROC curves.
Builds LDA/QDA model

Note: These are all a work in progress.  I'm not proposing any as a final model. But it's useful to have the code worked out while we try things

To do:
Try adding complexity
Work on normal distribution of continuous variables for LDA/QDA

```{r}
library(tidyverse)
# Bank Data Set
############## Description ###################
# The data is related with direct marketing campaigns of a Portuguese banking institution. 
# The marketing campaigns were based on phone calls. 
# Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be (or not) subscribed.
# From May 2008 to November 2010
# Goal: predict if client will subscribe a term deposit
# 45211 obs of 17 variables
# Description of variables:
##### Bank Client Data #######
# 1 - age (numeric)
# 2 - job : type of job (categorical: "admin.","unknown","unemployed","management","housemaid","entrepreneur","student","blue-collar","self-employed","retired","technician","services") 
# 3 - marital : marital status (categorical: "married","divorced","single"; note: "divorced" means divorced or widowed)
# 4 - education (categorical: "unknown","secondary","primary","tertiary")
# 5 - default: has credit in default? (binary: "yes","no")
# 6 - balance: average yearly balance, in euros (numeric) 
# 7 - housing: has housing loan? (binary: "yes","no")
# 8 - loan: has personal loan? (binary: "yes","no")
######### related with the last contact of the current campaign:######
# 9 - contact: contact communication type (categorical: "unknown","telephone","cellular") 
# 10 - day: last contact day of the month (numeric)
# 11 - month: last contact month of year (categorical: "jan", "feb", "mar", ..., "nov", "dec")
# 12 - duration: last contact duration, in seconds (numeric)
######## other attributes:#######
# 13 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)
# 14 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric, -1 means client was not previously contacted)
# 15 - previous: number of contacts performed before this campaign and for this client (numeric)
# 16 - poutcome: outcome of the previous marketing campaign (categorical: "unknown","other","failure","success")
#########Output variable (desired target):#######
# 17 - y - has the client subscribed a term deposit? (binary: "yes","no")

# Load CSV file:
# bank.full.csv file
bank.full <- read.csv("~/Downloads/bank/bank-full.csv", sep=";", stringsAsFactors=TRUE)
# Because these seem to be sequentially ordered, make a ID number in order
bank.full = bank.full %>%
  mutate(id = row_number())

# Now add in the year variable
bank.full = bank.full %>%
  mutate(year = case_when(id>=1 & id<=27729 ~ '2008',
                          id>=27730 & id<=42591 ~ '2009',
                          id>=42592 ~ '2010'))
bank.full$year = factor(bank.full$year)

# Let's reorder month so it makes sense on graphs
bank.full$month = factor(bank.full$month, levels=c("jan","feb","mar","apr","may","jun","jul","aug","sep","oct","nov","dec"))

# Change pdays -1 to 0:
bank.full$pdays[bank.full$pdays==-1] = 0

# Remove outlier (other in 2008)
bank.full = subset(bank.full, previous != 275)

# Remove 2008
bank.full.new = bank.full %>%
  filter(year != 2008)
# Drop level 2008
bank.full.new$year = droplevels(bank.full.new$year)
```

```{r}
# train test split
# 80/20 would be: 13985:3496
set.seed(1234)
index<-sample(1:dim(bank.full.new)[1],3496,replace=F)
test<-bank.full.new[index,]
train<-bank.full.new[-index,]
```

```{r}
# Build a logistic regression using glm() with all predictors to start
# Code from AutoClassify.R
library(ResourceSelection)
# Using glm
model.main<-glm(y ~ age+job+marital+education+default+balance+housing+loan+contact+day+month+duration+campaign+pdays+previous+poutcome+year, data=train,family = binomial(link="logit"))
(vif(model.main)[,3])^2
#Hosmer Lemeshow test for lack of fit.  Use as needed.  The g=10 is an option that deals with the continuous predictors if any are there.
#This should be increased with caution. 
hoslem.test(model.main$y, fitted(model.main), g=10)
# rejects. But the sample size is large, so I don't think it's an issue
#Summary of current fit
summary(model.main)
# Using the summary coefficients we can generate CI for each one in the table
exp(cbind("Odds ratio" = coef(model.main), confint.default(model.main, level = 0.95)))
# AIC: 10333
```

```{r}
#This starts with a null model and then builds up using forward selection up to all the predictors that were specified in main model previously.
# Code from AutoClassify.R
model.null<-glm(y ~ 1, data=train,family = binomial(link="logit"))
step(model.null,
     scope = list(upper=model.main),
     direction="forward",
     test="Chisq",
     data=train)
# Final model: y ~ month + duration + poutcome + year + housing + contact + day + education + campaign + loan + job + age
```

```{r}
# Code from AutoClassify.R
# Take a look at the result
# Final model: y ~ month + duration + poutcome + year + housing + contact + day + education + campaign + loan + job + age
model.forward.1=glm(y~month + duration + poutcome + year + housing + contact + day + education + campaign + loan + job + age, data=train,family=binomial(link="logit"))
summary(model.forward.1)
exp(cbind("Odds ratio" = coef(model.forward.1), confint.default(model.forward.1, level = 0.95)))
vif(model.forward.1)
plot(model.forward.1)
# AIC: 10326
```

```{r}
# Code from HW 12
# Try stepwise selection using AIC as stopping criterion
library(MASS)
library(tidyverse)
full.log<-glm(y~age+job+marital+education+default+balance+housing+loan+contact+day+month+duration+campaign+pdays+previous+poutcome+year,family="binomial",data=train)
step.log<-full.log %>% stepAIC(trace=FALSE)
coef(step.log)
summary(step.log)
exp(cbind("Odds ratio" = coef(step.log), confint.default(step.log, level = 0.95)))
vif(step.log)
# model: y ~ age + job + education + housing + loan + contact + day + month + duration + campaign + poutcome + year
# same as above
# AIC: 10326
```

```{r}
# Code from HW 12
# Try Forward Selection
forward.log<-full.log %>% stepAIC(direction="forward",trace=FALSE)
coef(forward.log)
summary(forward.log)
exp(cbind("Odds ratio" = coef(forward.log), confint.default(forward.log, level = 0.95)))
vif(forward.log)
# model: y ~ age + job + marital + education + default + balance + housing + loan + contact + day + month + duration + campaign + pdays + previous + poutcome + year
# basically everything except response and id (which isn't included in any model, just for my reference)
# AIC: 10333
```

```{r}
# Code from HW 12
# Try Backward Selection
# I think backward is the default in stepAIC()
backward.log<-full.log %>% stepAIC(direction="backward",trace=FALSE)
coef(backward.log)
summary(backward.log)
exp(cbind("Odds ratio" = coef(backward.log), confint.default(backward.log, level = 0.95)))
vif(backward.log)
# model: y ~ age + job + education + housing + loan + contact + day + month + duration + campaign + poutcome + year
# AIC: 10326
```

```{r}
# Code from HW 12
# Try Both Selection
both.log<-full.log %>% stepAIC(direction="both",trace=FALSE)
coef(both.log)
summary(both.log)
exp(cbind("Odds ratio" = coef(both.log), confint.default(both.log, level = 0.95)))
vif(both.log)
# model: y ~ age + job + education + housing + loan + contact + day + month + duration + campaign + poutcome + year
# same as above
# AIC: 10326
```

```{r}
# Code from HW 12
# Try LASSO
# Cross validation is used to obtain the optimal penalty value.  A final refit using the entire data set can then be obtained once the optimal penalty value is determined.  For this example, the object "finalmodel" produces the final lasso model.
library(glmnet)
dat.train.x <- model.matrix(y~age+job+marital+education+default+balance+housing+loan+contact+day+month+duration+campaign+pdays+previous+poutcome+year-1,train)
dat.train.y<-train$y
cvfit <- cv.glmnet(dat.train.x, dat.train.y, family = "binomial", type.measure = "class", nlambda = 1000)
plot(cvfit)
coef(cvfit, s = "lambda.min")
#CV misclassification error rate is 0.1717555
print("CV Error Rate:")
cvfit$cvm[which(cvfit$lambda==cvfit$lambda.min)]

#Optimal penalty
print("Penalty Value:")
cvfit$lambda.min

#For final model predictions go ahead and refit lasso using entire data set
finalmodel<-glmnet(dat.train.x, dat.train.y, family = "binomial",lambda=cvfit$lambda.min)
# age + job + marital + education + balance + housing + loan + contact + day + month + duration + campaign +  poutcome + year
# basically all but previous, pdays, and default
```

```{r}
# Code from glmnet_lda_code2_wRF.R
library(ROCR)
# NOTE: This is the ROC curve and predictions on the TRAIN data from the LASSO model above "finalmodel"

#Get training set predictions...We know they are biased but lets create ROC's.
#These are predicted probabilities from logistic model  exp(b)/(1+exp(b))
fit.pred <- predict(cvfit, newx = dat.train.x, type = "response")

#Compare the prediction to the real outcome
head(fit.pred)
head(dat.train.y)

#Create ROC curves
pred <- prediction(fit.pred[,1], dat.train.y)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")
auc.train <- auc.train@y.values

#Plot ROC
plot(roc.perf)
abline(a=0, b= 1) #Ref line indicating poor performance
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
# AUC 0.869
```

```{r}
# Code from HW12
# Make predictions on test set using LASSO model
dat.test.x<-model.matrix(y~age+job+marital+education+default+balance+housing+loan+contact+day+month+duration+campaign+pdays+previous+poutcome+year-1,test)
dat.test.y<-test$y
fit.pred.lasso <- predict(finalmodel, newx = dat.test.x, type = "response")

test$y[1:15]
fit.pred.lasso[1:15]

#Making predictions for stepwise as well
fit.pred.step<-predict(step.log,newdata=test,type="response")
```

```{r}
# Code from HW12
# These are error metrics for TEST set with LASSO and Stepwise models

#Lets use the predicted probabilities to classify the observations and make a final confusion matrix for the two models.  We can use it to calculate error metrics.
#Lets use a cutoff of 0.5 to make the classification.
cutoff<-0.5
class.lasso<-factor(ifelse(fit.pred.lasso>cutoff,"yes","no"),levels=c("no","yes"))
class.step<-factor(ifelse(fit.pred.step>cutoff,"yes","no"),levels=c("no","yes"))

#Confusion Matrix for Lasso
conf.lasso<-table(class.lasso,test$y)
print("Confusion matrix for LASSO")
conf.lasso

conf.step<-table(class.step,test$y)
print("Confusion matrix for Stepwise")
conf.step

#Accuracy of LASSO and Stepwise
print("Overall accuracy for LASSO and Stepwise respectively")
sum(diag(conf.lasso))/sum(conf.lasso)
sum(diag(conf.step))/sum(conf.step)

print("Alternative calculations of accuracy")
#Rather than making the calculations from the table, we can compute them more quickly using the following code which just checks if the prediction matches the truth and then computes the proportion.
mean(class.lasso==test$y)
mean(class.step==test$y)

# LASSO:
#class.lasso   no  yes
#        no  2537  435
#        yes  179  345
# overall accuracy: 0.8243707

# Stepwise:
#class.step   no  yes
#       no  2529  429
#       yes  187  351
# overall accuracy: 0.8237986

# Obviously these need some adjustment (ability to predict yes is not great) - look into better cutoff in ROC Curve
```

```{r}
# Code from glmnet_lda_code2_wRF.R
# ROC for test set

#ROC curves: LASSO model on test set
pred1 <- prediction(fit.pred.lasso[,1], dat.test.y)
roc.perf1 = performance(pred1, measure = "tpr", x.measure = "fpr")
auc.val1 <- performance(pred1, measure = "auc")
auc.val1 <- auc.val1@y.values
plot(roc.perf1, colorize=TRUE)
abline(a=0, b= 1)
text(x = .40, y = .6,paste("AUC = ", round(auc.val1[[1]],3), sep = ""))
# AUC=0.873.  Is there a problem?  I thought the training set should get a better AUC?

# Code from HW 12
#ROC curves: Step model on test set
results.step<-prediction(fit.pred.step, test$y,label.ordering=c("no","yes"))
roc.step = performance(results.step, measure = "tpr", x.measure = "fpr")
auc.val.step = performance(results.step, measure="auc")
auc.val.step = auc.val.step@y.values
plot(roc.step,colorize = TRUE)
abline(a=0, b= 1)
text(x = .40, y = .6,paste("AUC = ", round(auc.val.step[[1]],3), sep = ""))
# AUC=0.874
```

```{r}
# A look at different cut-off and accuracy metrics:
cutoff<-0.2
class.lasso<-factor(ifelse(fit.pred.lasso>cutoff,"yes","no"),levels=c("no","yes"))
class.step<-factor(ifelse(fit.pred.step>cutoff,"yes","no"),levels=c("no","yes"))

#Confusion Matrix for Lasso
conf.lasso<-table(class.lasso,test$y)
print("Confusion matrix for LASSO")
conf.lasso

conf.step<-table(class.step,test$y)
print("Confusion matrix for Stepwise")
conf.step

#Accuracy of LASSO and Stepwise
print("Overall accuracy for LASSO and Stepwise respectively")
sum(diag(conf.lasso))/sum(conf.lasso)
sum(diag(conf.step))/sum(conf.step)

# This number we choose depends on what percentage of yes's we want, obviously
```

```{r}
# Code from glmnet_lda_code2_wRF.R
# NOTE: We need to work on normality. But I thought I'd get the code set up.
## LDA

#Training Set
lda.train.x <- train[,c(1,6,10,12,13,14,15)]
lda.train.y <- train$y

fit.lda <- lda(lda.train.y ~ ., data = lda.train.x)
pred.lda <- predict(fit.lda, newdata = lda.train.x)

preds <- pred.lda$posterior
preds <- as.data.frame(preds)

# ROC for TRAINING data
pred <- prediction(preds[,2],lda.train.y)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf, colorize=TRUE)
abline(a=0, b= 1)
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
# AUC=0.76. Not surprising. Maybe better if we normalize and then weed out some variables

# Try QDA
# Double check this Nicole
fit.qda <- qda(lda.train.y ~ ., data = lda.train.x)
pred.qda <- predict(fit.qda, newdata = lda.train.x)

preds <- pred.qda$posterior
preds <- as.data.frame(preds)

# ROC for TRAINING data
pred <- prediction(preds[,2],lda.train.y)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf, colorize=TRUE)
abline(a=0, b= 1)
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
# AUC: 0.73
```

```{r}
# Code from glmnet_lda_code2_wRF.R
#LDA for TEST data
lda.test.x <- test[,c(1,6,10,12,13,14,15)]
lda.test.y <- test$y

pred.lda1 <- predict(fit.lda, newdata = lda.test.x)

preds1 <- pred.lda1$posterior
preds1 <- as.data.frame(preds1)

pred1 <- prediction(preds1[,2],lda.test.y)
roc.perf = performance(pred1, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred1, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf, colorize=TRUE)
abline(a=0, b= 1)
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
# AUC = 0.775

# QDA for TEST data
pred.qda1 <- predict(fit.qda, newdata = lda.test.x)

preds1 <- pred.qda1$posterior
preds1 <- as.data.frame(preds1)

pred1 <- prediction(preds1[,2],lda.test.y)
roc.perf = performance(pred1, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred1, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf, colorize=TRUE)
abline(a=0, b= 1)
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
# AUC = 0.731
```

```{r}
# try some interactions
model.complex<-glm(y ~ age+job+marital+education+default+balance+housing+loan+contact+day+month+duration+campaign+pdays+previous+poutcome+year+year:month+year:day, data=train,family = binomial(link="logit"))
step(model.main,
     scope = list(upper=model.complex),
     direction="forward",
     test="Chisq",
     data=newAuto)
# Success! looks like yearxday and yearxmonth were significant
# AIC: 10010
```

```{r}
# Code from glmnet_lda_code2_wRF.R
library(randomForest)
# Random Forest (training data)

# Remove id variable as it's just for reference
dat.train.rf <- train[,-18]

train.rf<-randomForest(y~.,data=dat.train.rf,mtry=4,ntree=500,importance=T)
fit.pred<-predict(train.rf,newdata=dat.train.rf,type="prob")

pred <- prediction(fit.pred[,2], dat.train.rf$y)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf)
abline(a=0, b= 1)
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
```

```{r}
# Code from glmnet_lda_code2_wRF.R
# Random Forest (test data)

#Predict test set
dat.val1.rf <- test[,-18]

pred.val1<-predict(train.rf,newdata=dat.val1.rf,type="prob")

pred <- prediction(pred.val1[,2], dat.val1.rf$y)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf, colorize=TRUE)
abline(a=0, b= 1)
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
# AUC = 0.897
```